{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda022b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.2.4, while the latest version is 1.3.5.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "import random\n",
    "import argparse\n",
    "import gc\n",
    "import torch\n",
    "import resource\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ogb.nodeproppred import Evaluator\n",
    "from utils import SimpleDataset\n",
    "from model import ClassMLP\n",
    "from utils import *\n",
    "from glob import glob\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Dataset and Algorithom\n",
    "    parser.add_argument('--seed', type=int, default=20159, help='random seed..')\n",
    "    parser.add_argument('--dataset', default='arxiv', help='dateset.')\n",
    "    # Algorithm parameters\n",
    "    parser.add_argument('--alpha', type=float, default=0.1, help='alpha.')\n",
    "    parser.add_argument('--rmax', type=float, default=1e-7, help='threshold.')\n",
    "    # Learining parameters\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, help='learning rate.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0, help='weight decay.')\n",
    "    parser.add_argument('--layer', type=int, default=4, help='number of layers.')\n",
    "    parser.add_argument('--hidden', type=int, default=1024, help='hidden dimensions.')\n",
    "    parser.add_argument('--dropout', type=float, default=0.3, help='dropout rate.')\n",
    "    parser.add_argument('--bias', default='none', help='bias.')\n",
    "    parser.add_argument('--epochs', type=int, default=1000, help='number of epochs.')\n",
    "    parser.add_argument('--batch', type=int, default=10000, help='batch size.')\n",
    "    parser.add_argument('--patience', type=int, default=50, help='patience.')\n",
    "    parser.add_argument('--dev', type=int, default=1, help='device id.')\n",
    "#     args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    print(\"--------------------------\")\n",
    "    print(args)\n",
    "    checkpt_file = 'pretrained/'+uuid.uuid4().hex+'.pt'\n",
    "\n",
    "    features,train_labels,val_labels,test_labels,train_idx,val_idx,test_idx,memory_dataset, py_alg = load_ogb_init(args.dataset, args.alpha,args.rmax) ##\n",
    "    prepare_to_train(features, train_idx, val_idx, test_idx, train_labels, val_labels, test_labels, args, checkpt_file)\n",
    "    print('------------------ update -------------------')\n",
    "    snapList = [f for f in glob('./data/'+args.dataset+'/*Edgeupdate_snap*.txt')]\n",
    "    print('number of snapshots: ', len(snapList))\n",
    "    for i in range(len(snapList)):\n",
    "        py_alg.snapshot_operation('data/'+args.dataset+'/'+args.dataset+'_Edgeupdate_snap'+str(i+1)+'.txt', args.rmax, args.alpha, features)\n",
    "        prepare_to_train(features, train_idx, val_idx, test_idx, train_labels, val_labels, test_labels, args, checkpt_file)\n",
    "\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    time_epoch=0\n",
    "    loss_list=[]\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        t_st=time.time()\n",
    "        x, y = x.cuda(device), y.cuda(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = F.nll_loss(out, y.squeeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        time_epoch+=(time.time()-t_st)\n",
    "        loss_list.append(loss.item())\n",
    "    return np.mean(loss_list),time_epoch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, device, loader, evaluator):\n",
    "    model.eval()\n",
    "    y_pred, y_true = [], []\n",
    "    for step,(x,y) in enumerate(loader):\n",
    "        x = x.cuda(device)\n",
    "        out = model(x)\n",
    "        y_pred.append(torch.argmax(out, dim=1, keepdim=True).cpu())\n",
    "        y_true.append(y)\n",
    "    return evaluator.eval({\n",
    "        \"y_true\": torch.cat(y_true, dim=0),\n",
    "        \"y_pred\": torch.cat(y_pred, dim=0),\n",
    "    })['acc']\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, device, loader, evaluator,checkpt_file):\n",
    "    model.load_state_dict(torch.load(checkpt_file))\n",
    "    model.eval()\n",
    "    y_pred, y_true = [], []\n",
    "    for step,(x,y) in enumerate(loader):\n",
    "        x = x.cuda(device)\n",
    "        out = model(x)\n",
    "        y_pred.append(torch.argmax(out, dim=1, keepdim=True).cpu())\n",
    "        y_true.append(y)\n",
    "    return evaluator.eval({\n",
    "        \"y_true\": torch.cat(y_true, dim=0),\n",
    "        \"y_pred\": torch.cat(y_pred, dim=0),\n",
    "    })['acc']\n",
    "\n",
    "def prepare_to_train(features, train_idx, val_idx, test_idx, train_labels, val_labels, test_labels, args, checkpt_file):\n",
    "    features = torch.FloatTensor(features)\n",
    "    features_train = features[train_idx]\n",
    "    features_val = features[val_idx]\n",
    "    features_test = features[test_idx]\n",
    "    del features\n",
    "    gc.collect()\n",
    "\n",
    "    label_dim = int(max(train_labels.max(),val_labels.max(),test_labels.max()))+1\n",
    "    train_dataset = SimpleDataset(features_train,train_labels)\n",
    "    valid_dataset = SimpleDataset(features_val,val_labels)\n",
    "    test_dataset = SimpleDataset(features_test, test_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch,shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    model = ClassMLP(features_train.size(-1),args.hidden,label_dim,args.layer,args.dropout).cuda(args.dev)\n",
    "    evaluator = Evaluator(name='ogbn-papers100M')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    bad_counter = 0\n",
    "    best = 0\n",
    "    best_epoch = 0\n",
    "    train_time = 0\n",
    "    model.reset_parameters()\n",
    "    print(\"--------------------------\")\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(args.epochs):\n",
    "        loss_tra,train_ep = train(model,args.dev,train_loader,optimizer)\n",
    "        t_st=time.time()\n",
    "        f1_val = validate(model, args.dev, valid_loader, evaluator)\n",
    "        train_time+=train_ep\n",
    "        if(epoch+1)%20 == 0:\n",
    "            print(f'Epoch:{epoch+1:02d},'\n",
    "            f'Train_loss:{loss_tra:.3f}',\n",
    "            f'Valid_acc:{100*f1_val:.2f}%',\n",
    "            f'Time_cost:{train_ep:.3f}/{train_time:.3f}')\n",
    "        if f1_val > best:\n",
    "            best = f1_val\n",
    "            best_epoch = epoch+1\n",
    "            t_st=time.time()\n",
    "            torch.save(model.state_dict(), checkpt_file)\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "        if bad_counter == args.patience:\n",
    "            break\n",
    "\n",
    "    test_acc = test(model, args.dev, test_loader, evaluator,checkpt_file)\n",
    "    print(f\"Train cost: {train_time:.2f}s\")\n",
    "    print('Load {}th epoch'.format(best_epoch))\n",
    "    print(f\"Test accuracy:{100*test_acc:.2f}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958472f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python instant",
   "language": "python",
   "name": "instant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
